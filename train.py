import os
import time
import argparse
import torch
from torch.utils.data import DataLoader
# from model.cnn import Net as Model
# from model.resnet3d import resnet50 as Model
from model.explainmodel import ExplainablesMRIModel as Model
# from model.efficientnet3D import efficientnetv2_m as Model
# from model.vgg3d import vgg as Model
from dataset import MyDataset
from utils import get_params_groups, train_one_epoch_freeze_masks, validate_freeze_masks
from utils import cosine_lr_scheduler as create_lr_scheduler
from sklearn.model_selection import train_test_split
import pandas as pd
import pickle
import numpy as np
import random
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from scipy.ndimage import binary_dilation, generate_binary_structure

import scipy.stats as stats
import torch.nn.functional as F
import torch
import torch.nn.functional as F
import SimpleITK as sitk
import torch
import torch.nn.functional as F


def check_gradient_stability(gradient_history, model, window_size=3, threshold=0.95):
    """
    æ£€æŸ¥Maskæ¢¯åº¦åœ¨æœ€è¿‘å‡ ä¸ªepochæ˜¯å¦ç¨³å®šã€‚

    Args:
        gradient_history (list): å­˜å‚¨æ¯ä¸ªepochçš„`epoch_gradient_stats`çš„åˆ—è¡¨ã€‚
        model: åŒ…å«masksçš„æ¨¡å‹å¯¹è±¡
        window_size (int): ç”¨äºæ¯”è¾ƒçš„è¿ç»­epochæ•°é‡ã€‚
        threshold (float): æ¢¯åº¦ç¬¦å·ä¸€è‡´æ€§æ¯”ä¾‹çš„é˜ˆå€¼ï¼Œé«˜äºæ­¤å€¼è¢«è®¤ä¸ºç¨³å®šã€‚

    Returns:
        bool: å¦‚æœåœ¨çª—å£æœŸå†…æ‰€æœ‰ç›¸é‚»epochçš„å¹³å‡ä¸€è‡´æ€§éƒ½é«˜äºé˜ˆå€¼ï¼Œåˆ™è¿”å›Trueã€‚
    """
    # è‡³å°‘éœ€è¦çª—å£å¤§å°çš„æ¢¯åº¦å†å²è®°å½•æ‰èƒ½è¿›è¡Œæ¯”è¾ƒ
    if len(gradient_history) < window_size:
        return False

    # è·å–å½“å‰çš„masks
    current_masks = [mask.data.clone().to('cpu') for mask in model.masks]

    # è·å–æœ€è¿‘çš„æ¢¯åº¦è®°å½•
    recent_grads = gradient_history[-window_size:]

    all_pair_consistencies = []
    all_union_ratios = []

    # æ¯”è¾ƒçª—å£å†…çš„æ¯ä¸€å¯¹ç›¸é‚»çš„epochæ¢¯åº¦
    for i in range(window_size - 1):
        epoch_grads_1 = recent_grads[i]
        epoch_grads_2 = recent_grads[i + 1]

        # è®¡ç®—è¿™ä¸€å¯¹epochä¸­æ‰€æœ‰maskçš„å¹³å‡ä¸€è‡´æ€§
        current_pair_consistencies = []
        current_union_ratios = []

        # éå†æ‰€æœ‰masks
        for j in range(len(current_masks)):
            mask_name = f'mask_{j}'

            # ç¡®ä¿ä¸¤ä¸ªepochéƒ½æœ‰è¯¥maskçš„æ¢¯åº¦
            if mask_name not in epoch_grads_1 or mask_name not in epoch_grads_2:
                continue

            grad1 = epoch_grads_1[mask_name]['mean_gradient']
            grad2 = epoch_grads_2[mask_name]['mean_gradient']

            # è·å–å¯¹åº”çš„mask
            current_mask = current_masks[j]

            # è·å–æ¯ä¸ªç‚¹çš„æ¢¯åº¦ç¬¦å· (+1, -1, or 0)
            signs1 = torch.sign(grad1)
            signs2 = torch.sign(grad2)

            # è®¡ç®—æ€»ç‚¹æ•°
            total_points = grad1.numel()

            # è®¡ç®—ä¸€è‡´æ€§æ¯”ä¾‹
            if total_points == 0:
                consistency_ratio = 1.0
                union_ratio = 1.0
            else:
                # åˆ†åˆ«è®¡ç®—ä¸¤ä¸ªæ¢¯åº¦çš„å‰90%é‡è¦ç‚¹
                abs_grad1 = torch.abs(grad1).flatten()
                abs_grad2 = torch.abs(grad2).flatten()

                # å¯¹grad1æ‰¾å‰90%ç‚¹
                sorted_abs1, indices1 = torch.sort(abs_grad1, descending=True)
                cumsum1 = torch.cumsum(sorted_abs1, dim=0)
                total_sum1 = cumsum1[-1]
                threshold_90_1 = total_sum1 * 1
                mask_90_1 = cumsum1 <= threshold_90_1
                if not mask_90_1.any():
                    num_selected1 = 1
                else:
                    num_selected1 = mask_90_1.sum().item() + 1
                    num_selected1 = min(num_selected1, len(indices1))
                important_indices1 = set(indices1[:num_selected1].tolist())

                # å¯¹grad2æ‰¾å‰90%ç‚¹
                sorted_abs2, indices2 = torch.sort(abs_grad2, descending=True)
                cumsum2 = torch.cumsum(sorted_abs2, dim=0)
                total_sum2 = cumsum2[-1]
                threshold_90_2 = total_sum2 * 1
                mask_90_2 = cumsum2 <= threshold_90_2
                if not mask_90_2.any():
                    num_selected2 = 1
                else:
                    num_selected2 = mask_90_2.sum().item() + 1
                    num_selected2 = min(num_selected2, len(indices2))
                important_indices2 = set(indices2[:num_selected2].tolist())

                # å–ä¸¤ä¸ªé›†åˆçš„å¹¶é›†
                union_indices = important_indices1.union(important_indices2)
                union_indices = torch.tensor(list(union_indices))

                # è®¡ç®—å¹¶é›†å æ€»ç‚¹æ•°çš„æ¯”ä¾‹
                union_ratio = len(union_indices) / total_points

                # åªåœ¨è¿™äº›é‡è¦ç‚¹ä¸Šè®¡ç®—ç¬¦å·ä¸€è‡´æ€§
                signs1_flat = signs1.flatten()
                signs2_flat = signs2.flatten()

                signs1_important = signs1_flat[union_indices]
                signs2_important = signs2_flat[union_indices]

                # --- ä¿®æ”¹ï¼šåªåœ¨maskä¸º1çš„ä½ç½®è®¡ç®—ä¸€è‡´æ€§æ¯”ä¾‹ ---
                # è·å–maskå¹¶å±•å¹³
                mask_flat = current_mask.flatten()
                mask_important = mask_flat[union_indices]

                # æ‰¾å‡ºmaskä¸º1çš„ä½ç½®
                mask_ones_positions = mask_important == 1
                num_mask_ones = mask_ones_positions.sum().item()

                if num_mask_ones == 0:
                    # å¦‚æœé‡è¦ç‚¹ä¸­æ²¡æœ‰maskä¸º1çš„ä½ç½®ï¼Œè®¤ä¸ºæ˜¯ä¸€è‡´çš„
                    consistency_ratio = 1.0
                else:
                    # åªåœ¨maskä¸º1çš„ä½ç½®è®¡ç®—ç¬¦å·ä¸€è‡´æ€§
                    signs1_mask_ones = signs1_important[mask_ones_positions]
                    signs2_mask_ones = signs2_important[mask_ones_positions]

                    num_consistent_points = (signs1_mask_ones == signs2_mask_ones).sum().item()
                    consistency_ratio = num_consistent_points / num_mask_ones

            current_pair_consistencies.append(consistency_ratio)
            current_union_ratios.append(union_ratio)

        if not current_pair_consistencies:
            continue

        # è®¡ç®—è¿™å¯¹epochçš„å¹³å‡ä¸€è‡´æ€§
        avg_consistency = sum(current_pair_consistencies) / len(current_pair_consistencies)
        avg_union_ratio = sum(current_union_ratios) / len(current_union_ratios)
        all_pair_consistencies.append(avg_consistency)
        all_union_ratios.append(avg_union_ratio)

    # å¦‚æœæ‰€æœ‰ç›¸é‚»å¯¹çš„å¹³å‡ä¸€è‡´æ€§éƒ½é«˜äºé˜ˆå€¼ï¼Œåˆ™è®¤ä¸ºæ¢¯åº¦å·²ç¨³å®š
    if not all_pair_consistencies:  # å¦‚æœæ²¡æœ‰ä»»ä½•å¯æ¯”è¾ƒçš„æ¢¯åº¦
        return False

    # æ£€æŸ¥ä¸€è‡´æ€§å’Œå¹¶é›†å æ¯”ä¸¤ä¸ªæ¡ä»¶
    consistency_stable = all(consist >= threshold for consist in all_pair_consistencies)
    union_stable = all(union_ratio >= 0.95 for union_ratio in all_union_ratios)
    is_stable = consistency_stable and union_stable

    # æ‰“å°ä¿¡æ¯
    if is_stable:
        print(f"ğŸ“ˆ æ¢¯åº¦ç¬¦å·å·²ç¨³å®š! æœ€è¿‘ {window_size - 1} æ¬¡epoché—´å¹³å‡ä¸€è‡´æ€§: " +
              ", ".join([f"{c:.4f}" for c in all_pair_consistencies]) +
              f" (é˜ˆå€¼ > {threshold}) | å¹¶é›†å æ¯”: " +
              ", ".join([f"{r:.4f}" for r in all_union_ratios]) +
              " (é˜ˆå€¼ >0.95)")
    else:
        print(f"ğŸ“‰ æ¢¯åº¦ç¬¦å·æœªç¨³å®šã€‚æœ€è¿‘ {window_size - 1} æ¬¡epoché—´å¹³å‡ä¸€è‡´æ€§: " +
              ", ".join([f"{c:.4f}" for c in all_pair_consistencies]) +
              f" (é˜ˆå€¼ > {threshold}) | å¹¶é›†å æ¯”: " +
              ", ".join([f"{r:.4f}" for r in all_union_ratios]) +
              " (é˜ˆå€¼ > 0.95)")

    return is_stable
def load_two_class_data(root, subject, data_csv, max_samples_per_class=30):
    """
    å¿«é€ŸåŠ è½½ä¸¤ç±»æ•°æ®ï¼ˆå‡å°‘æ ·æœ¬æ•°ï¼Œéšæœºé‡‡æ ·ï¼‰
    """
    print(f"ğŸ—‚ï¸ å¿«é€ŸåŠ è½½ä¸¤ç±»æ•°æ®...")

    # åˆ›å»ºæ•°æ®é›†
    train_dataset = MyDataset(root, subject, data_csv, train=True)

    # é¢„å…ˆæ”¶é›†å„ç±»æ ·æœ¬çš„ç´¢å¼•
    class1_indices = []
    class2_indices = []

    print("ğŸ“‹ æ‰«ææ•°æ®é›†æ ‡ç­¾...")
    for i in range(len(train_dataset)):
        # åªè¯»å–æ ‡ç­¾ï¼Œä¸è¯»å–æ•°æ®
        _, label = train_dataset.sample[i], train_dataset.label[i]
        if label == 0:
            class1_indices.append(i)
        elif label == 1:
            class2_indices.append(i)

    # éšæœºé‡‡æ ·
    class1_indices = random.sample(class1_indices, min(max_samples_per_class, len(class1_indices)))
    class2_indices = random.sample(class2_indices, min(max_samples_per_class, len(class2_indices)))

    print(f"ğŸ“¥ åŠ è½½ AD: {len(class1_indices)} æ ·æœ¬, MCI: {len(class2_indices)} æ ·æœ¬")

    # æ‰¹é‡åŠ è½½æ•°æ®
    class1_data = []
    class2_data = []

    # åŠ è½½class1æ•°æ®
    for i in class1_indices:
        try:
            arr, _ ,_= train_dataset[i]
            class1_data.append(arr)
        except Exception as e:
            print(f"è·³è¿‡æ ·æœ¬{i}: {e}")

    # åŠ è½½class2æ•°æ®
    for i in class2_indices:
        try:
            arr, _,_ = train_dataset[i]
            class2_data.append(arr)
        except Exception as e:
            print(f"è·³è¿‡æ ·æœ¬{i}: {e}")

    if len(class1_data) > 0 and len(class2_data) > 0:
        class1_data = torch.stack(class1_data)
        class2_data = torch.stack(class2_data)
        print(f"âœ… åŠ è½½å®Œæˆ: AD {class1_data.shape}, MCI {class2_data.shape}")
        return class1_data, class2_data
    else:
        raise ValueError("æ²¡æœ‰æ”¶é›†åˆ°è¶³å¤Ÿçš„æ•°æ®ï¼")


def generate_initial_masks_ttest(root, subject, data_csv):
    """
    å¿«é€Ÿç”Ÿæˆåˆå§‹maskï¼ˆå‘é‡åŒ–t-testï¼‰
    """
    print("ğŸš€ å¼€å§‹å¿«é€Ÿç”Ÿæˆåˆå§‹mask...")

    # 1. åŠ è½½æ•°æ®ï¼ˆå‡å°‘æ ·æœ¬æ•°ï¼‰
    class1_data, class2_data = load_two_class_data(root, subject, data_csv, max_samples_per_class=100)

    # 2. ç§»é™¤channelç»´åº¦
    if class1_data.dim() == 5 and class1_data.shape[1] == 1:
        class1_data = class1_data.squeeze(1).numpy()  # (n, D, H, W)
        class2_data = class2_data.squeeze(1).numpy()  # (n, D, H, W)

    print("ğŸ” è®¡ç®—voxel-wise t-test (å‘é‡åŒ–)...")

    # 3. å‘é‡åŒ–è®¡ç®—tå€¼
    shape = class1_data.shape[1:]  # (D, H, W)

    # reshapeä¸º (n_samples, n_voxels)
    class1_flat = class1_data.reshape(class1_data.shape[0], -1)  # (n1, D*H*W)
    class2_flat = class2_data.reshape(class2_data.shape[0], -1)  # (n2, D*H*W)

    # å‘é‡åŒ–t-test
    t_stats, p_values = stats.ttest_ind(class1_flat, class2_flat, axis=0)
    t_values = np.abs(t_stats)

    # å¤„ç†NaN
    t_values = np.nan_to_num(t_values, nan=0.0)

    print(f"ğŸ“Š tå€¼èŒƒå›´: [{np.min(t_values):.4f}, {np.max(t_values):.4f}]")

    # 4. ç”Ÿæˆmask
    sorted_indices = np.argsort(t_values)[::-1]  # é™åº
    total_voxels = len(t_values)

    # 5. ç”Ÿæˆ5ä¸ªmask: 50%, 40%, 30%, 20%, 10%
    percentages = [0.5, 0.4, 0.3, 0.2, 0.1]
    masks = []

    for i, p in enumerate(percentages):
        n_voxels = int(total_voxels * p)
        mask = np.zeros_like(t_values)
        mask[sorted_indices[:n_voxels]] = 1.0
        mask = mask.reshape(shape)  # æ¢å¤åŸå§‹å½¢çŠ¶
        masks.append(torch.tensor(mask, dtype=torch.float32))
        print(f"   Mask{i + 1} (top {p * 100:.0f}%): {n_voxels} voxels")

    print("âœ… å¿«é€Ÿmaskç”Ÿæˆå®Œæˆ!")
    return masks
def update_masks_from_gradients_no_expand(model, gradient_stats):
    """
    æ ¹æ®æ¢¯åº¦æ›´æ–°maskï¼Œä½†ä¸è¿›è¡Œæ‰©å±•æ“ä½œ
    """
    print("ğŸ”„ æ ¹æ®æ¢¯åº¦æ›´æ–°masks...")

    old_masks = [mask.data.clone() for mask in model.masks]
    new_masks = []
    mask_changes = []

    for i in range(5):
        mask_name = f'mask_{i}'
        if mask_name in gradient_stats:
            # è·å–å¹³å‡æ¢¯åº¦
            mean_gradient = gradient_stats[mask_name]['mean_gradient']

            # æ ¹æ®æ¢¯åº¦ç¬¦å·æ›´æ–°maskï¼šæ­£å€¼â†’1ï¼Œè´Ÿå€¼â†’0
            new_mask = torch.zeros_like(old_masks[i])
            new_mask[mean_gradient > 0] = 1.0
            new_mask[mean_gradient <= 0] = 0.0

            # è®¡ç®—å˜åŒ–ç‡ - ä¿®æ”¹ä¸ºé™¤ä»¥æ—§maskä¸­ä¸º1çš„ä½ç½®æ•°é‡
            active_positions = torch.sum(old_masks[i]).item()  # æ—§maskä¸­ä¸º1çš„ä½ç½®æ•°
            if active_positions > 0:
                change_ratio = torch.sum(torch.abs(new_mask - old_masks[i])) / active_positions
            else:
                change_ratio = 0.0  # å¦‚æœåŸæ¥æ²¡æœ‰æ¿€æ´»ä½ç½®ï¼Œå˜åŒ–ç‡ä¸º0

            new_masks.append(new_mask)
            mask_changes.append({
                'mask_idx': i + 1,
                'change_ratio': change_ratio.item() if isinstance(change_ratio, torch.Tensor) else change_ratio,
                'active_voxels_before': torch.sum(old_masks[i]).item(),
                'active_voxels_after': torch.sum(new_mask).item(),
            })

            print(f"   Mask{i + 1}: å˜åŒ–ç‡={change_ratio:.4f}, "
                  f"æ´»è·ƒä½“ç´ : {torch.sum(old_masks[i]).item()} â†’ {torch.sum(new_mask).item()}")
        else:
            # å¦‚æœæ²¡æœ‰æ¢¯åº¦ä¿¡æ¯ï¼Œä¿æŒåŸmask
            new_masks.append(old_masks[i])
            mask_changes.append({
                'mask_idx': i + 1,
                'change_ratio': 0.0,
                'active_voxels_before': torch.sum(old_masks[i]).item(),
                'active_voxels_after': torch.sum(old_masks[i]).item(),
            })

    new_masks = enforce_subset_constraint(new_masks, mode='shrink')
    # æ›´æ–°æ¨¡å‹ä¸­çš„masks
    for i, new_mask in enumerate(new_masks):
        model.masks[i].data.copy_(new_mask)

    return mask_changes


def adaptive_mask_expansion(model, gradient_stats, decay_rate=1.0, max_distance=3.0, gradient_weight=2.0):
    """
    è‡ªé€‚åº”çš„Maskæ‰©å±•ï¼šæ ¹æ®æ¢¯åº¦ä¿¡æ¯æŒ‡å¯¼æ‰©å±•æ–¹å‘ã€‚

    Args:
        model (nn.Module): ä½ çš„æ¨¡å‹å®ä¾‹ã€‚
        gradient_stats (dict): åŒ…å«æ¯ä¸ªmaskå¹³å‡æ¢¯åº¦çš„å­—å…¸ã€‚
        decay_rate (float): è·ç¦»è¡°å‡ç‡ï¼Œè¶Šå¤§åˆ™è·ç¦»æƒ©ç½šè¶Šé‡ã€‚
        max_distance (float): æœ€å¤§è€ƒè™‘çš„æ‰©å±•è·ç¦»ã€‚
        gradient_weight (float): æ¢¯åº¦ä¿¡æ¯çš„æƒé‡ï¼Œè¶Šå¤§åˆ™æ¢¯åº¦å¯¹æ‰©å±•æ–¹å‘çš„å½±å“è¶Šå¤§ã€‚
    """
    print("ğŸ”§ è¿›è¡Œè‡ªé€‚åº”Maskæ‰©å±• (Adaptive Mask Expansion)...")

    # è·å–å½“å‰çš„maskå’Œå¹³å‡æ¢¯åº¦
    current_masks = [mask.data.clone() for mask in model.masks]

    expanded_masks = []

    for i in range(len(current_masks)):

        mask_name = f'mask_{i}'
        if mask_name not in gradient_stats or gradient_stats[mask_name] is None:
            print(f"   Mask{i + 1}: ç¼ºå°‘æ¢¯åº¦ä¿¡æ¯ï¼Œè·³è¿‡æ‰©å±•ã€‚")
            expanded_masks.append(current_masks[i])
            continue

        current_mask = current_masks[i]
        mean_gradient = gradient_stats[mask_name]['mean_gradient']
        if i == 4:
            if torch.sum(current_mask)<300:
                decay_rate = 0.8
                max_distance = 3.0
                gradient_weight = 2.0
        # 1. è¯†åˆ«å‰æ²¿åŒºåŸŸ (Find the frontier)
        # å°†maskè½¬æ¢ä¸ºnumpyè¿›è¡Œè†¨èƒ€æ“ä½œ
        mask_np = current_mask.cpu().numpy().astype(bool)
        # 3Dçš„ç»“æ„å…ƒç´  (6-è¿é€š)
        struct = generate_binary_structure(3, 1)
        # è†¨èƒ€ä¸€å±‚
        dilated_mask_np = binary_dilation(mask_np, structure=struct)
        # è½¬æ¢å›tensor
        dilated_mask = torch.from_numpy(dilated_mask_np).float().to(current_mask.device)
        # å‰æ²¿å°±æ˜¯è†¨èƒ€åçš„åŒºåŸŸå‡å»åŸå§‹åŒºåŸŸ
        frontier = F.relu(dilated_mask - current_mask)

        # å¦‚æœæ²¡æœ‰å‰æ²¿ï¼ˆæ•´ä¸ªç©ºé—´éƒ½æ»¡äº†ï¼‰ï¼Œåˆ™ä¸æ‰©å±•
        if torch.sum(frontier) == 0:
            expanded_masks.append(current_mask)
            continue

        # 2. è®¡ç®—æ‰©å±•æ¦‚ç‡
        # a) åŸºäºè·ç¦»çš„æ¦‚ç‡
        distance_field = compute_distance_field(current_mask)
        # åªåœ¨æœ€å¤§è·ç¦»å†…è€ƒè™‘
        valid_region = (distance_field <= max_distance)
        proximity_prob = torch.exp(-decay_rate * distance_field) * valid_region.float()

        # b) åŸºäºæ¢¯åº¦çš„å¢ç›Š
        # åªè€ƒè™‘æ­£æ¢¯åº¦ï¼ˆæœ‰æ½œåŠ›çš„åŒºåŸŸï¼‰ï¼Œå¹¶è¿›è¡Œå½’ä¸€åŒ–å¤„ç†
        positive_gradient = F.relu(mean_gradient)
        # å½’ä¸€åŒ–æ¢¯åº¦ï¼Œä½¿å…¶å½±å“æ›´ç¨³å®š
        if positive_gradient.max() > 0:
            normalized_gradient = positive_gradient / positive_gradient.max()
        else:
            normalized_gradient = torch.zeros_like(positive_gradient)

        # æ¢¯åº¦å¢ç›Šå› å­ï¼šåŸºç¡€æ˜¯1ï¼Œæœ‰æ­£æ¢¯åº¦çš„åŒºåŸŸä¼šå¾—åˆ°åŠ æˆ
        gradient_gain = 1.0 + gradient_weight * normalized_gradient

        # c) è®¡ç®—æœ€ç»ˆæ‰©å±•æ¦‚ç‡ï¼ˆåªåœ¨å‰æ²¿åŒºåŸŸè®¡ç®—ï¼‰
        final_expansion_prob = proximity_prob.to("cpu") * gradient_gain.to("cpu") * frontier.to("cpu")

        # 3. æ¦‚ç‡é‡‡æ ·
        random_field = torch.rand_like(final_expansion_prob)
        newly_activated_voxels = (random_field < final_expansion_prob).float()

        # 4. æ›´æ–°Mask
        expanded_mask = current_mask.to("cpu") + newly_activated_voxels.to("cpu")
        expanded_masks.append(expanded_mask)

        print(
            f"   Mask{i + 1} æ‰©å±•: {torch.sum(current_mask).item():.0f} â†’ {torch.sum(expanded_mask).item():.0f} ä¸ªæ´»è·ƒä½“ç´ ")

    # ç¡®ä¿æ‰©å±•åä»æ»¡è¶³å­é›†å…³ç³»
    enforced_expanded_masks = enforce_subset_constraint(expanded_masks, mode='expand')

    # æ›´æ–°æ¨¡å‹ä¸­çš„masks
    for i, new_mask in enumerate(enforced_expanded_masks):
        model.masks[i].data.copy_(new_mask)

    print("âœ… è‡ªé€‚åº”æ‰©å±•å®Œæˆ!")

def expand_masks(model, decay_rate=0.8, max_distance=3.0,mode=None):
    """
    å¯¹æ¨¡å‹ä¸­çš„æ‰€æœ‰maskè¿›è¡Œæ¦‚ç‡åœºæ‰©å±•æ“ä½œ
    """
    print("ğŸ”§ è¿›è¡Œæ¦‚ç‡åœºæ‰©å±•æ“ä½œ...")

    expanded_masks = []
    for i, mask in enumerate(model.masks):
        expanded_mask = probability_field_expansion(mask.data, decay_rate, max_distance)
        expanded_masks.append(expanded_mask)
        print(f"   Mask{i + 1}æ‰©å±•: {torch.sum(mask.data).item()} â†’ {torch.sum(expanded_mask).item()}ä¸ªæ´»è·ƒä½“ç´ ")

    # ç¡®ä¿æ‰©å±•åä»æ»¡è¶³å­é›†å…³ç³»
    enforced_expanded_masks = enforce_subset_constraint(expanded_masks,mode=mode)

    # æ›´æ–°æ¨¡å‹ä¸­çš„masks
    for i, expanded_mask in enumerate(enforced_expanded_masks):
        model.masks[i].data.copy_(expanded_mask)


def probability_field_expansion(mask, decay_rate=0.8, max_distance=3.0):
    """
    æ¦‚ç‡åœºæ‰©å±•ï¼šåŸºäºè·ç¦»çš„æ¦‚ç‡è¡°å‡æ¥å†³å®šæ‰©å±•

    Args:
        mask: åŸå§‹mask
        decay_rate: æ¦‚ç‡è¡°å‡ç‡ï¼Œè¶Šå¤§è¡°å‡è¶Šå¿«
        max_distance: æœ€å¤§æ‰©å±•è·ç¦»ï¼Œè¶…è¿‡æ­¤è·ç¦»æ¦‚ç‡ä¸º0
    """
    # è®¡ç®—è·ç¦»åœº
    distance_field = compute_distance_field(mask)

    # åªè€ƒè™‘åœ¨max_distanceèŒƒå›´å†…çš„ç‚¹
    valid_region = distance_field <= max_distance

    # ç”Ÿæˆæ¦‚ç‡åœºï¼šP(x) = exp(-decay_rate * distance(x))
    probability_field = torch.exp(-decay_rate * distance_field) * valid_region.float()

    # ç”Ÿæˆéšæœºåœºè¿›è¡Œæ¦‚ç‡æŠ½æ ·
    random_field = torch.rand_like(probability_field)

    # å†³å®šæ‰©å±•ï¼šåªå¯¹émaskåŒºåŸŸè¿›è¡Œæ‰©å±•åˆ¤æ–­
    non_mask_region = (1 - mask)
    expansion_decisions = (random_field < probability_field) * non_mask_region

    # åˆå¹¶ï¼šåŸå§‹mask + æ¦‚ç‡æ‰©å±•çš„åŒºåŸŸ
    expanded_mask = mask + expansion_decisions

    return expanded_mask


def compute_distance_field(mask):
    """
    è®¡ç®—è·ç¦»åœºï¼šæ¯ä¸ªç‚¹åˆ°æœ€è¿‘maskç‚¹çš„è·ç¦»
    """
    from scipy.ndimage import distance_transform_edt

    # è½¬æ¢ä¸ºnumpy
    mask_np = mask.cpu().numpy()

    # è®¡ç®—è·ç¦»å˜æ¢ï¼ˆåˆ°æœ€è¿‘maskç‚¹çš„è·ç¦»ï¼‰
    distance_np = distance_transform_edt(1 - mask_np)

    # è½¬æ¢å›tensor
    distance_field = torch.from_numpy(distance_np).float().to(mask.device)

    return distance_field


# ä¿æŒä½ åŸæœ‰çš„å‡½æ•°
def expand_mask_3d(mask, expand_size=2):
    """
    3D maskæ‰©å±•ï¼šå€¼ä¸º1çš„ç‚¹å‘å¤–æ‰©å±•expand_sizeä¸ªvoxel
    """
    # è½¬æ¢ä¸ºnumpyè¿›è¡Œå¤„ç†
    mask_np = mask.cpu().numpy()

    # åˆ›å»ºç»“æ„å…ƒç´ ï¼ˆ6-è¿é€šï¼‰
    struct = generate_binary_structure(3, 1)

    # è¿›è¡Œå¤šæ¬¡è†¨èƒ€
    expanded_np = mask_np.copy()
    for _ in range(expand_size):
        expanded_np = binary_dilation(expanded_np, structure=struct)

    # è½¬æ¢å›tensor
    expanded_mask = torch.from_numpy(expanded_np.astype(np.float32)).to(mask.device)

    return expanded_mask


def enforce_subset_constraint(masks, mode='shrink'):
    """
    ç¡®ä¿maskæ»¡è¶³å­é›†å…³ç³»

    Args:
        masks: åŸå§‹maskåˆ—è¡¨
        mode: 'shrink' æ”¶ç¼©æ¨¡å¼ æˆ– 'expand' æ‰©å¼ æ¨¡å¼
    """
    if len(masks) <= 1:
        return masks

    if mode == 'shrink':
        if len(masks) <= 1:
            return masks

        print("   ğŸ”— æ‰§è¡Œé€æ­¥æ”¶ç¼©çº¦æŸ...")
        enforced_masks = []

        # ç¬¬ä¸€ä¸ªmaskä¿æŒä¸å˜ï¼ˆä¸»å¯¼maskï¼‰
        enforced_masks.append(masks[0])
        print(f"      Mask1: ä¿æŒä¸å˜ï¼Œæ´»è·ƒä½“ç´ æ•° = {torch.sum(masks[0]).item()}")

        # åç»­maskä¸å‰ä¸€ä¸ªmaskæ±‚äº¤é›†
        for i in range(1, len(masks)):
            # å½“å‰maskä¸å‰ä¸€ä¸ªenforced maskæ±‚äº¤é›†
            enforced_mask = masks[i] * enforced_masks[i - 1]
            enforced_masks.append(enforced_mask)

            # ç»Ÿè®¡å˜åŒ–
            original_count = torch.sum(masks[i]).item()
            enforced_count = torch.sum(enforced_mask).item()
            removed_count = original_count - enforced_count

            if removed_count > 0:
                print(f"      Mask{i + 1}: æ±‚äº¤é›†åï¼Œç§»é™¤äº†{removed_count}ä¸ªç‚¹ ({original_count} â†’ {enforced_count})")
            else:
                print(f"      Mask{i + 1}: æ— å˜åŒ–ï¼Œæ´»è·ƒä½“ç´ æ•° = {enforced_count}")

    elif mode == 'expand':
        print("   ğŸ”— æ‰§è¡Œé€æ­¥æ‰©å¼ çº¦æŸ...")
        enforced_masks = [mask.clone() for mask in masks]

        # ä»æœ€ç»†çš„mask(mask5, index=4)å¼€å§‹ï¼Œå‘å‰ç¡®ä¿å­é›†å…³ç³»
        for i in range(4, 0, -1):  # i: 4,3,2,1 å¯¹åº”mask5,4,3,2
            mask_fine = enforced_masks[i]  # æ›´ç»†çš„mask
            mask_coarse = enforced_masks[i - 1]  # æ›´ç²—çš„mask

            # ç¡®ä¿ç»†maskä¸º1çš„åœ°æ–¹ï¼Œç²—maskä¹Ÿä¸º1
            enforced_masks[i - 1] = torch.maximum(mask_coarse, mask_fine)

        print(f"      å­é›†çº¦æŸåæ´»è·ƒä½“ç´ æ•°: " +
              " â†’ ".join([f"M{i + 1}:{torch.sum(mask).item()}" for i, mask in enumerate(enforced_masks)]))

    return enforced_masks


def check_convergence(mask_changes, threshold=0.01):
    """
    æ£€æŸ¥maskæ˜¯å¦æ”¶æ•›
    """
    if not mask_changes:
        return False

    avg_change = sum([change['change_ratio'] for change in mask_changes]) / len(mask_changes)
    return avg_change < threshold


def main(args):
    # è®¾ç½®éšæœºç§å­
    seed = 1121
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True

    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(args.save, exist_ok=True)

    # å‡†å¤‡æ•°æ®
    train_set = MyDataset(args.root, args.subject, args.data_csv, train=True)
    test_set = MyDataset(args.root, args.subject, args.data_csv, train=False)
    test_set = test_set+train_set
    train_loader = DataLoader(train_set, batch_size=args.train_batch, shuffle=False)
    test_loader = DataLoader(test_set, batch_size=args.test_batch, shuffle=False)
    aal = sitk.ReadImage('aal113.nii')
    aal = sitk.GetArrayFromImage(aal)
    aal = torch.tensor(aal, dtype=torch.float32).to(args.device)
    aal[aal>0] = 1
    # è·å–æ•°æ®å½¢çŠ¶ï¼ˆå‡è®¾æ•°æ®æ˜¯3Dçš„ï¼‰
    sample_data, _ ,_= train_set[0]
    if len(sample_data.shape) == 4:  # (C, D, H, W)
        input_shape = sample_data.shape[1:]  # (D, H, W)
    else:  # (D, H, W)
        input_shape = sample_data.shape

    print(f"æ•°æ®å½¢çŠ¶: {input_shape}")

    # ç”Ÿæˆåˆå§‹mask
    initial_masks = generate_initial_masks_ttest(args.root, args.subject, args.data_csv)
    # åˆ›å»ºæ¨¡å‹
    model = Model(input_shape=input_shape, initial_masks=initial_masks).to(args.device)
    with torch.no_grad():  # å¦‚æœä¸éœ€è¦æ¢¯åº¦
        for i in range(5):
            model.masks[i].data = model.masks[i].data * aal
    '''state_dict = torch.load('save/explainable_mri/cn_mci.pt', weights_only=True,map_location='cpu')
    #model.masks.load_state_dict(state_dict)
    for param, saved_mask in zip(model.masks, state_dict):
        param.data.copy_(saved_mask)'''
    '''if "cuda" in args.device and torch.cuda.device_count() > 1:
        model = torch.nn.DataParallel(model)'''


    state_dict = torch.load('save/explainable_mri/best_cn_mci_kaiming_uniform_1.532_bn_2.pt', weights_only=True,map_location='cpu')
    model.load_state_dict(state_dict)
    scaler = torch.cuda.amp.GradScaler() if args.amp else None
    if args.amp:
        print("å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP)")
    else:
        print("ä½¿ç”¨å¸¸è§„ç²¾åº¦è®­ç»ƒ")
    # ä¼˜åŒ–å™¨ã€å­¦ä¹ ç‡è°ƒåº¦å™¨å’ŒæŸå¤±å‡½æ•°


    print("ğŸš€ å¼€å§‹é€epochæ¸è¿›å¼Maskå­¦ä¹ ...")
    print("=" * 80)

    converged = False
    mask_change = 0
    while 1:
        model=model.to(args.device)
        gradient_history = []
        if converged:
            break
        params_group = get_params_groups(model, weight_decay=args.wd)
        optimizer = torch.optim.SGD(params_group, lr=args.lr, weight_decay=args.wd,momentum=0.9)
        lr_scheduler = create_lr_scheduler(optimizer, len(train_loader), args.epoch, args.warmup)
        criterion = torch.nn.CrossEntropyLoss()

        # æ—¥å¿—è®°å½•
        start_time = time.time()
        logger = {
            "train": {"loss": [], "acc": [], "sen": [], "spec": [], "f1": [], "auc": []},
            "test": {"loss": [], "acc": [], "sen": [], "spec": [], "f1": [], "auc": []},
            "epoch_mask_updates": [],  # è®°å½•æ¯ä¸ªepochçš„maskæ›´æ–°
            "convergence_info": {}
        }
        for epoch in range(args.epoch):

            test_metrics = validate_freeze_masks(
                model, test_loader, criterion, args.device,
                scaler if args.amp else None  # ä¼ å…¥scaler
            )
            print(f"\n--- Epoch {epoch + 1}/{args.epoch} ---")

            # 1. è®­ç»ƒä¸€ä¸ªå®Œæ•´çš„epoch
            train_loss, epoch_gradient_stats, train_metrics = train_one_epoch_freeze_masks(
                model, train_loader, optimizer, criterion, args.device,
                scaler if args.amp else None,
                lr_scheduler
            )
            epoch_gradient_stats = {k: {'mean_gradient': -v['mean_gradient']} for k, v in epoch_gradient_stats.items()}
            gradient_history.append(epoch_gradient_stats)


            # è®°å½•è®­ç»ƒæŒ‡æ ‡
            logger["train"]["loss"].append(train_loss)
            logger["train"]["acc"].append(train_metrics["accuracy"])
            logger["train"]["sen"].append(train_metrics["recall"])
            logger["train"]["spec"].append(train_metrics["precision"])
            logger["train"]["f1"].append(train_metrics["f1_score"])
            logger["train"]["auc"].append(train_metrics["auc"])

            # 2. æµ‹è¯•å½“å‰epochçš„æ€§èƒ½
            test_metrics = validate_freeze_masks(
                model, test_loader, criterion, args.device,
                scaler if args.amp else None  # ä¼ å…¥scaler
            )
            logger["test"]["loss"].append(test_metrics["loss"])
            logger["test"]["acc"].append(test_metrics["accuracy"])
            logger["test"]["sen"].append(test_metrics["recall"])
            logger["test"]["spec"].append(test_metrics["precision"])
            logger["test"]["f1"].append(test_metrics["f1_score"])
            logger["test"]["auc"].append(test_metrics["auc"])
            should_update_mask = False
            if epoch >= 1500:  # è‡³å°‘5ä¸ªepochåæ‰å¼€å§‹åˆ¤æ–­
                # æ¡ä»¶2ï¼šæ£€æŸ¥æ¢¯åº¦æ˜¯å¦ç¨³å®š
                gradient_is_stable = check_gradient_stability(
                    gradient_history,
                    model,
                    window_size=3,  # æ¯”è¾ƒæœ€è¿‘3ä¸ªepoch
                    threshold=0.99  # ç›¸ä¼¼åº¦é˜ˆå€¼è®¾ä¸º95%
                )

                if  gradient_is_stable:
                    should_update_mask = True

                    print(f"ğŸ¯ è§¦å‘Maskæ›´æ–°")
            if should_update_mask:
                # 3. æ ¹æ®æ¢¯åº¦æ›´æ–°mask (ä¸è¿›è¡Œæ‰©å±•)
                if epoch_gradient_stats:
                    mask_changes = update_masks_from_gradients_no_expand(
                        model, epoch_gradient_stats
                    )
                else:
                    print("âš ï¸ æ²¡æœ‰æ¢¯åº¦ä¿¡æ¯ï¼Œè·³è¿‡maskæ›´æ–°")
                    mask_changes = []

                # 4. æ£€æŸ¥æ”¶æ•›
                if check_convergence(mask_changes, args.convergence_threshold):
                    print(f"ğŸ¯ Maskå·²æ”¶æ•›ï¼è®­ç»ƒåœæ­¢äºç¬¬{epoch + 1}ä¸ªepoch")
                    converged = True
                    logger["convergence_info"] = {
                        "converged": True,
                        "final_epoch": epoch + 1,
                        "reason": "mask_convergence"
                    }
                    break
                else:

                    # 5. ä¸æ”¶æ•› â†’ è¿›è¡Œæ‰©å±•æ“ä½œ
                    if mask_changes:  # åªæœ‰å½“æœ‰maskå˜åŒ–æ—¶æ‰è¾“å‡ºå˜åŒ–ç‡
                        avg_change = sum([c['change_ratio'] for c in mask_changes]) / len(mask_changes)
                        print(f"Maskå¹³å‡å˜åŒ–ç‡: {avg_change:.4f} > {args.convergence_threshold}, æœªæ”¶æ•›")
                    # è¿›è¡Œæ‰©å±•æ“ä½œ
                    #expand_masks(model, decay_rate=2, max_distance=3.0,mode='expand')
                    '''adaptive_mask_expansion(
                        model,
                        epoch_gradient_stats,  # <--- ä¼ å…¥æ¢¯åº¦ç»Ÿè®¡ä¿¡æ¯
                        decay_rate=1.5,  # å¯è°ƒè¶…å‚æ•°
                        max_distance=3,  # å¯è°ƒè¶…å‚æ•°
                        gradient_weight=2 # <--- å…³é”®è¶…å‚æ•°ï¼šæ§åˆ¶æ¢¯åº¦çš„å½±å“åŠ›
                    )'''
                    with torch.no_grad():  # å¦‚æœä¸éœ€è¦æ¢¯åº¦
                        for i in range(5):
                            model.masks[i].data = model.masks[i].data * aal
                    model.init_classify_module('kaiming_uniform')
                    print("æ›´æ–°maskï¼Œå¼€å§‹æ–°çš„ä¸€è½®è®­ç»ƒ...")
                    mask_change += 1
                    current_round_best_acc = max(logger["test"]["acc"]) if logger["test"]["acc"] else 0
                    print(f"ç¬¬{mask_change}æ¬¡maskæ›´æ–°ï¼Œå½“å‰è½®æ¬¡æœ€ä½³å‡†ç¡®ç‡: {current_round_best_acc:.4f}")
                    if mask_change == 2:
                        torch.save(current_masks, args.save + f"cn_mci.pt")
                    break

            # 6. è®°å½•è¿™æ¬¡epochçš„ä¿¡æ¯
            logger["epoch_mask_updates"].append({
                "epoch": epoch + 1,
                "train_metrics": train_metrics,
                "test_metrics": test_metrics,
                "converged": converged,
                "expanded": not converged  # è®°å½•æ˜¯å¦è¿›è¡Œäº†æ‰©å±•
            })

            # 7. ä¿å­˜æ¨¡å‹å’Œæ—¥å¿—
            #torch.save(model.state_dict(), args.save + f'epoch_{epoch + 1}_cn_mci_kaiming_uniform_1.532_bn_2.pt')
            if test_metrics["accuracy"] >= max(logger["test"]["acc"]):
                torch.save(model.state_dict(), args.save + 'best_cn_mci_kaiming_uniform_1.532_bn_2.pt')
                print(f"ğŸ¯ æ–°çš„æœ€ä½³å‡†ç¡®ç‡: {test_metrics['accuracy']:.4f}")

            # ä¿å­˜å½“å‰maskçŠ¶æ€
            current_masks = model.get_masks()
            torch.save(current_masks, args.save + f"masks_cn_mci_kaiming_uniform_1.532_bn_2.pt")
            torch.save(logger, args.save + "logger_cn_mci_kaiming_uniform_1.532_bn_2.pt")

        # è®­ç»ƒå®Œæˆ
        if not converged:
            logger["convergence_info"] = {
                "converged": False,
                "reason": "max_epochs_reached"
            }

        logger["time"] = time.time() - start_time
        torch.save(logger, args.save + "logger_cn_mci_kaiming_uniform_1.532_bn_2.pt")

    print("\n" + "=" * 80)
    print("âœ… è®­ç»ƒå®Œæˆ!")
    print(f"æ€»è®­ç»ƒæ—¶é—´: {logger['time']:.2f}ç§’")
    print(f"æœ€ç»ˆæœ€ä½³å‡†ç¡®ç‡: {max(logger['test']['acc']):.4f}")
    if logger["convergence_info"]["converged"]:
        print(f"æ”¶æ•›äºç¬¬ {logger['convergence_info']['final_epoch']} ä¸ªepoch")
    else:
        print("æœªè¾¾åˆ°æ”¶æ•›ï¼Œå·²å®Œæˆæ‰€æœ‰epoch")
    print("=" * 80)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--epoch", default=500, type=int, help="æ€»epochæ•°")
    parser.add_argument("--warmup", default=1, type=int, help="warmup epochs")
    parser.add_argument("--train_batch", default=128, type=int, help="train batch size")
    parser.add_argument("--test_batch", default=16, type=int, help="test batch size")
    parser.add_argument("--num_classes", default=2, type=int, help="class number")
    parser.add_argument("--lr", default=1e-4, type=float, help="learning rate")
    parser.add_argument("--wd", default=5e-3, type=float, help="weight decay")
    parser.add_argument("--amp", default=True, type=bool, help="use amp")
    parser.add_argument("--device", default="cuda:5", type=str, help="device")
    parser.add_argument("--root", default="./dataset/BrainClass/", type=str, help="dataset root")
    parser.add_argument("--subject", default="./files/subject_all.pt", type=str, help="subject file")
    parser.add_argument("--data_csv", default="./files/data.csv", type=str, help="data csv")
    parser.add_argument("--save", default="./save/explainable_mri/", type=str, help="save path")

    # Maskç›¸å…³å‚æ•°
    parser.add_argument("--convergence_threshold", default=0.001, type=float, help="maskæ”¶æ•›é˜ˆå€¼")
    parser.add_argument("--expand_size", default=2, type=int, help="maskæ‰©å±•å¤§å°")

    args = parser.parse_args()

    torch.backends.cudnn.benchmark = True
    main(args)