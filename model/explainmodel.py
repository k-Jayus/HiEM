import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Function


class SparseConv3dFunction(Function):
    @staticmethod
    def forward(ctx, input, weight, bias, mask):
        """
        è¶…ç¨€ç–ä¼˜åŒ–ï¼šåªå¯¹éé›¶ä½ç½®è®¡ç®—
        """
        # ğŸš€ æ‰¾åˆ°çœŸæ­£çš„æœ‰æ•ˆä½ç½®
        effective_positions = ((input.abs() > 1e-8) * mask).nonzero(as_tuple=False)

        if len(effective_positions) == 0:
            # æ²¡æœ‰æœ‰æ•ˆä½ç½®ï¼Œç›´æ¥è¿”å›é›¶
            batch_size = input.size(0)
            output = torch.zeros(batch_size, weight.size(0), 1, 1, 1,
                                 device=input.device, dtype=input.dtype)
            ctx.save_for_backward(input, weight, mask)
            ctx.effective_positions = effective_positions
            return output

        # ğŸš€ åªæå–æœ‰æ•ˆä½ç½®çš„å€¼
        effective_values = input[effective_positions[:, 0],
        effective_positions[:, 1],
        effective_positions[:, 2],
        effective_positions[:, 3],
        effective_positions[:, 4]]

        effective_weights = weight[:, 0,
                            effective_positions[:, 2],
                            effective_positions[:, 3],
                            effective_positions[:, 4]]

        # ğŸš€ ç¨€ç–çŸ©é˜µä¹˜æ³•
        batch_size = input.size(0)
        n_channels = weight.size(0)
        output = torch.zeros(batch_size, n_channels, 1, 1, 1,
                             device=input.device, dtype=input.dtype)

        # æŒ‰batchåˆ†ç»„è®¡ç®—
        for b in range(batch_size):
            batch_mask = effective_positions[:, 0] == b
            if batch_mask.any():
                batch_values = effective_values[batch_mask]
                batch_weights = effective_weights[:, batch_mask]

                # çŸ©é˜µä¹˜æ³•ï¼š[n_channels, n_positions] Ã— [n_positions] = [n_channels]
                output[b, :, 0, 0, 0] = torch.mv(batch_weights, batch_values)

        if bias is not None:
            output += bias.view(1, -1, 1, 1, 1)

        ctx.save_for_backward(input, weight, bias, mask)
        ctx.effective_positions = effective_positions
        return output

    @staticmethod
    def backward(ctx, grad_output):
        """
        è¶…ç¨€ç–åå‘ä¼ æ’­ï¼šåªå¯¹æœ‰æ•ˆä½ç½®è®¡ç®—
        """
        input, weight, bias, mask = ctx.saved_tensors
        effective_positions = ctx.effective_positions

        grad_input = grad_weight = grad_bias = grad_mask = None

        if len(effective_positions) == 0:
            # æ²¡æœ‰æœ‰æ•ˆä½ç½®ï¼Œè¿”å›é›¶æ¢¯åº¦
            if ctx.needs_input_grad[0]:
                grad_input = torch.zeros_like(input)
            if ctx.needs_input_grad[1]:
                grad_weight = torch.zeros_like(weight)
            if ctx.needs_input_grad[2]:
                grad_bias = torch.zeros_like(bias) if bias is not None else None
            if ctx.needs_input_grad[3]:
                grad_mask = torch.zeros_like(mask)
            return grad_input, grad_weight, grad_bias, grad_mask

        # ğŸš€ åªå¯¹æœ‰æ•ˆä½ç½®è®¡ç®—æ¢¯åº¦
        batch_size = input.size(0)
        n_channels = weight.size(0)

        # è¾“å…¥æ¢¯åº¦
        if ctx.needs_input_grad[0]:
            grad_input = torch.zeros_like(input)

            for b in range(batch_size):
                batch_mask = effective_positions[:, 0] == b
                if batch_mask.any():
                    batch_positions = effective_positions[batch_mask]
                    batch_weights = weight[:, 0,
                                    batch_positions[:, 2],
                                    batch_positions[:, 3],
                                    batch_positions[:, 4]]

                    # åå‘ä¼ æ’­ï¼š[n_channels] Ã— [n_channels, n_positions] = [n_positions]
                    grad_values = torch.mv(batch_weights.t(), grad_output[b, :, 0, 0, 0])

                    # æ•£å¸ƒå›åŸå§‹ä½ç½®
                    grad_input[batch_positions[:, 0],
                    batch_positions[:, 1],
                    batch_positions[:, 2],
                    batch_positions[:, 3],
                    batch_positions[:, 4]] = grad_values

        # æƒé‡æ¢¯åº¦
        if ctx.needs_input_grad[1]:
            grad_weight = torch.zeros_like(weight)

            for b in range(batch_size):
                batch_mask = effective_positions[:, 0] == b
                if batch_mask.any():
                    batch_positions = effective_positions[batch_mask]
                    batch_values = input[batch_positions[:, 0],
                    batch_positions[:, 1],
                    batch_positions[:, 2],
                    batch_positions[:, 3],
                    batch_positions[:, 4]]

                    # å¤–ç§¯ï¼š[n_channels, 1] Ã— [1, n_positions] = [n_channels, n_positions]
                    grad_batch = torch.outer(grad_output[b, :, 0, 0, 0], batch_values)

                    # æ•£å¸ƒå›åŸå§‹ä½ç½®
                    grad_weight[:, 0,
                    batch_positions[:, 2],
                    batch_positions[:, 3],
                    batch_positions[:, 4]] += grad_batch

        # biasæ¢¯åº¦
        if ctx.needs_input_grad[2] and bias is not None:
            grad_bias = grad_output.sum(dim=(0, 2, 3, 4))

        # maskæ¢¯åº¦
        if ctx.needs_input_grad[3]:
            grad_mask = torch.zeros_like(mask)

            for b in range(batch_size):
                batch_mask = effective_positions[:, 0] == b
                if batch_mask.any():
                    batch_positions = effective_positions[batch_mask]

                    # è®¡ç®—maskæ¢¯åº¦
                    input_values = input[batch_positions[:, 0],
                    batch_positions[:, 1],
                    batch_positions[:, 2],
                    batch_positions[:, 3],
                    batch_positions[:, 4]]

                    weight_values = weight[:, 0,
                                    batch_positions[:, 2],
                                    batch_positions[:, 3],
                                    batch_positions[:, 4]]

                    # æ¢¯åº¦è®¡ç®—
                    grad_values = torch.mv(weight_values.t(), grad_output[b, :, 0, 0, 0]) * input_values

                    # æ•£å¸ƒå›åŸå§‹ä½ç½®
                    grad_mask[0, 0,
                    batch_positions[:, 2],
                    batch_positions[:, 3],
                    batch_positions[:, 4]] += grad_values

        return grad_input, grad_weight, grad_bias, grad_mask

class OptimizedClassifyModule(nn.Module):
    def __init__(self, input_shape):
        super().__init__()
        D, H, W = input_shape

        # å·ç§¯æƒé‡å’Œåç½®
        self.weight = nn.Parameter(torch.randn(2, 1, D, H, W) * 0.01)
        self.bias = nn.Parameter(torch.zeros(2))

        # MLPåˆ†ç±»å™¨
        self.mlp = nn.Sequential(
            nn.Flatten(),
            #nn.BatchNorm1d(2),

            #nn.Linear(2, 2)
        )

    def forward(self, x, mask):
        """
        Args:
            x: (batch_size, 1, D, H, W)
            mask: (1, 1, D, H, W)
        """
        # ä½¿ç”¨å¸¦æ¢¯åº¦çš„ç¨€ç–å·ç§¯
        conv_output = SparseConv3dFunction.apply(x, self.weight, self.bias, mask)
        return self.mlp(conv_output)


class ExplainablesMRIModel(nn.Module):
    def __init__(self, input_shape, initial_masks=None):
        super(ExplainablesMRIModel, self).__init__()

        self.input_shape = input_shape  # (D, H, W)

        # å®šä¹‰5ä¸ªå¯è®­ç»ƒçš„mask - ç¡®ä¿æ¢¯åº¦è®¡ç®—
        if initial_masks is not None:
            self.masks = nn.ParameterList([
                nn.Parameter(mask.clone().float(), requires_grad=True) for mask in initial_masks
            ])
            print(f"âœ… ä½¿ç”¨æä¾›çš„åˆå§‹maskï¼Œå…±{len(initial_masks)}ä¸ª")
        else:
            self.masks = nn.ParameterList([
                nn.Parameter(torch.ones(input_shape), requires_grad=True) for _ in range(5)
            ])
            print("âœ… ä½¿ç”¨é»˜è®¤å…¨1åˆå§‹mask")

        # æ‰“å°maskä¿¡æ¯
        for i, mask in enumerate(self.masks):
            print(f"   Mask{i + 1}: shape={mask.shape}, requires_grad={mask.requires_grad}")

        # å®šä¹‰åˆ†ç±»æ¨¡å—
        self.classify = self._make_classify_module(input_shape)

        print(f"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œè¾“å…¥å½¢çŠ¶: {input_shape}")

    def _make_classify_module(self, input_shape):
        return OptimizedClassifyModule(input_shape)

    def init_classify_module(self, init_method='xavier_normal'):
        """
        åˆå§‹åŒ–classifyæ¨¡å—çš„æƒé‡

        Args:
            init_method (str): åˆå§‹åŒ–æ–¹æ³•ï¼Œå¯é€‰ï¼š
                - 'xavier_normal': Xavieræ­£æ€åˆ†å¸ƒåˆå§‹åŒ–
                - 'xavier_uniform': Xavierå‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–
                - 'kaiming_normal': Kaimingæ­£æ€åˆ†å¸ƒåˆå§‹åŒ–
                - 'kaiming_uniform': Kaimingå‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–
                - 'default': é»˜è®¤åˆå§‹åŒ–(åŸæœ‰æ–¹æ³•)
        """
        with torch.no_grad():
            if init_method == 'xavier_normal':
                # Xavieræ­£æ€åˆ†å¸ƒåˆå§‹åŒ– - é€‚åˆtanh/sigmoidæ¿€æ´»å‡½æ•°
                nn.init.xavier_normal_(self.classify.weight)
                nn.init.zeros_(self.classify.bias)

            elif init_method == 'xavier_uniform':
                # Xavierå‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–
                nn.init.xavier_uniform_(self.classify.weight)
                nn.init.zeros_(self.classify.bias)

            elif init_method == 'kaiming_normal':
                # Kaimingæ­£æ€åˆ†å¸ƒåˆå§‹åŒ– - é€‚åˆReLUæ¿€æ´»å‡½æ•°
                nn.init.kaiming_normal_(self.classify.weight, mode='fan_out', nonlinearity='relu')
                nn.init.zeros_(self.classify.bias)

            elif init_method == 'kaiming_uniform':
                # Kaimingå‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–
                nn.init.kaiming_uniform_(self.classify.weight, mode='fan_out', nonlinearity='relu')
                nn.init.zeros_(self.classify.bias)

            elif init_method == 'default':
                # ä¿æŒåŸæœ‰çš„åˆå§‹åŒ–æ–¹æ³•
                D, H, W = self.input_shape
                self.classify.weight.data = torch.randn(2, 1, D, H, W) * 0.01
                self.classify.bias.data = torch.zeros(2)

            else:
                raise ValueError(f"ä¸æ”¯æŒçš„åˆå§‹åŒ–æ–¹æ³•: {init_method}")

        # åˆå§‹åŒ–MLPå±‚
        for module in self.classify.mlp.modules():
            if isinstance(module, nn.Linear):
                if init_method.startswith('xavier'):
                    nn.init.xavier_normal_(module.weight)
                elif init_method.startswith('kaiming'):
                    nn.init.kaiming_normal_(module.weight, nonlinearity='relu')
                else:
                    nn.init.normal_(module.weight, 0, 0.01)

                if module.bias is not None:
                    nn.init.zeros_(module.bias)

        print(f"âœ… Classifyæ¨¡å—æƒé‡åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ–¹æ³•: {init_method}")
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­ï¼šä¾æ¬¡ä½¿ç”¨5ä¸ªmaskè¿›è¡Œåˆ†ç±»

        Args:
            x: (batch_size, 1, D, H, W)

        Returns:
            List[Tensor]: 5ä¸ªåˆ†ç±»ç»“æœï¼Œæ¯ä¸ªshapeä¸º(batch_size, 2)
        """
        results = []
        current = x
        for i in range(5):
            # è·å–maskå¹¶æ‰©å±•ç»´åº¦ä»¥åŒ¹é…batch
            mask = self.masks[i].unsqueeze(0).unsqueeze(0)  # (1, 1, D, H, W)

            # ç¡®ä¿maskåœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            if mask.device != x.device:
                mask = mask.to(x.device)

            # ä½¿ç”¨å½“å‰maskè¿›è¡Œåˆ†ç±»
            result = self.classify(current, mask)
            results.append(result)
            current = current*mask

        return results

    def get_masks(self):
        """è¿”å›å½“å‰çš„maskå‚æ•°ï¼Œç”¨äºå¯è§†åŒ–"""
        return [mask.data.clone() for mask in self.masks]

    def set_mask(self, mask_idx, new_mask):
        """è®¾ç½®ç‰¹å®šçš„mask"""
        if 0 <= mask_idx < 5:
            with torch.no_grad():
                self.masks[mask_idx].copy_(new_mask)
        else:
            raise ValueError(f"mask_idxåº”è¯¥åœ¨0-4ä¹‹é—´ï¼Œå¾—åˆ°{mask_idx}")

    def freeze_masks(self):
        """å†»ç»“æ‰€æœ‰maskå‚æ•°"""
        for mask in self.masks:
            mask.requires_grad_(False)
        print("ğŸ”’ æ‰€æœ‰maskå·²å†»ç»“")

    def unfreeze_masks(self):
        """è§£å†»æ‰€æœ‰maskå‚æ•°"""
        for mask in self.masks:
            mask.requires_grad_(True)
        print("ğŸ”“ æ‰€æœ‰maskå·²è§£å†»")

    def get_mask_gradients(self):
        """è·å–maskæ¢¯åº¦ä¿¡æ¯"""
        grad_info = {}
        for i, mask in enumerate(self.masks):
            if mask.grad is not None:
                grad_info[f'mask_{i + 1}'] = {
                    'norm': mask.grad.norm().item(),
                    'mean': mask.grad.mean().item(),
                    'std': mask.grad.std().item(),
                    'max': mask.grad.max().item(),
                    'min': mask.grad.min().item()
                }
            else:
                grad_info[f'mask_{i + 1}'] = None
        return grad_info

    def print_model_info(self):
        """æ‰“å°æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)

        print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯:")
        print(f"   æ€»å‚æ•°: {total_params:,}")
        print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print(f"   è¾“å…¥å½¢çŠ¶: {self.input_shape}")
        print(f"   Maskæ•°é‡: {len(self.masks)}")

        # åˆ†åˆ«ç»Ÿè®¡maskå’Œåˆ†ç±»å™¨å‚æ•°
        mask_params = sum(mask.numel() for mask in self.masks)
        classify_params = sum(p.numel() for p in self.classify.parameters())

        print(f"   Maskå‚æ•°: {mask_params:,}")
        print(f"   åˆ†ç±»å™¨å‚æ•°: {classify_params:,}")


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
if __name__ == "__main__":
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # å‡è®¾sMRIçš„å½¢çŠ¶
    input_shape = (91, 109, 91)  # å…¸å‹çš„MNIç©ºé—´å¤§å°
    batch_size = 2

    # åˆ›å»ºæ¨¡å‹ï¼ˆä¸ä½¿ç”¨åˆå§‹maskï¼‰
    model = ExplainablesMRIModel(input_shape).to(device)
    model.print_model_info()

    # åˆ›å»ºç¤ºä¾‹è¾“å…¥
    x = torch.randn(batch_size, 1, *input_shape).to(device)
    print(f"\nğŸ“¥ è¾“å…¥æ•°æ®å½¢çŠ¶: {x.shape}")

    # å‰å‘ä¼ æ’­
    print("\nğŸ”„ å¼€å§‹å‰å‘ä¼ æ’­...")
    results = model(x)

    print(f"âœ… æ¨¡å‹è¾“å‡ºäº† {len(results)} ä¸ªç»“æœ")
    for i, result in enumerate(results):
        print(f"   ç»“æœ {i + 1} å½¢çŠ¶: {result.shape}")

    # æµ‹è¯•æ¢¯åº¦è®¡ç®—
    print("\nğŸ§® æµ‹è¯•æ¢¯åº¦è®¡ç®—...")
    criterion = nn.CrossEntropyLoss()
    targets = torch.randint(0, 2, (batch_size,)).to(device)

    total_loss = 0
    for i, result in enumerate(results):
        loss = criterion(result, targets)
        total_loss += loss

    print(f"æ€»æŸå¤±: {total_loss.item():.4f}")

    # åå‘ä¼ æ’­
    total_loss.backward()

    # æ£€æŸ¥maskæ¢¯åº¦
    print("\nğŸ“ˆ Maskæ¢¯åº¦ä¿¡æ¯:")
    grad_info = model.get_mask_gradients()
    for mask_name, info in grad_info.items():
        if info is not None:
            print(f"   {mask_name}: èŒƒæ•°={info['norm']:.6f}, å‡å€¼={info['mean']:.6f}")
        else:
            print(f"   {mask_name}: æ— æ¢¯åº¦")

    # æµ‹è¯•maskæ“ä½œ
    print("\nğŸ”§ æµ‹è¯•maskæ“ä½œ...")
    masks = model.get_masks()
    print(f"è·å–åˆ° {len(masks)} ä¸ªmaskï¼Œæ¯ä¸ªå½¢çŠ¶: {masks[0].shape}")

    # æµ‹è¯•å†»ç»“/è§£å†»
    model.freeze_masks()
    model.unfreeze_masks()

    print("\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")